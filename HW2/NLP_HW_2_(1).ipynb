{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Arsh misra"
      ],
      "metadata": {
        "id": "FSbx7HjON85-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import nltk\n",
        "drive.mount('/content/drive')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUE31UaPF5aH",
        "outputId": "0fd8cb87-17a8-4a0e-9a1d-8f73d61fb948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I decided to use google drive on colab because spyder does not work for me. The content of the files  is the same aside from the input text, which I will list in full in this notebook."
      ],
      "metadata": {
        "id": "SLO-PA4om-PN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the directory\n",
        "path = '/content/drive/My Drive/NLPHWPOSNEG/'\n",
        "\n",
        "# List files\n",
        "files = os.listdir(path)\n",
        "print(files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp3RXEC8F8F_",
        "outputId": "dfb27d13-e878-490d-9f88-7c98a203e6c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['positive-words.txt', 'input-text2.txt', 'negative-words.txt', 'example-text.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Create a function called gen_senti that tokenizes arbritrary text and compares each token with the positive and negative lexicons of each dictionary and ouputs the sentiment score S. Positive and negative words, pw and nw, count as a score of 1 and -1, respectively, for each word matched. The total count for pw and nw are pc and nc, respectively. Each message sentiment, S, is normalized between -1 and 1. Any text that does not have any positive or negative words would have been ignored, and not scored."
      ],
      "metadata": {
        "id": "A4gbREikHZO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a different example since the question did not specify that we needed to use the original example. The input text is listed as:\n",
        "\n",
        "\"Karl Peterson has been living the cruel inverse of the American dream. His rent keeps getting higher, but his apartments keep getting smaller. Peterson left the Midwest nine years ago for the epicenter of an economic boom, only to gradually learn that endless sunshine and desert views are increasingly among the few bargains left in Arizona. Peterson married his wife and they struggled to save for a home, moving through four apartments as their rent nearly tripled from $625 to $1,800 a month\"\n",
        "\n",
        "This is an article taken from yahoo finance:\n",
        "https://finance.yahoo.com/news/american-despair-arizona-high-home-151927862.html"
      ],
      "metadata": {
        "id": "ax6CnxVsKdNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "with open('/content/drive/My Drive/NLPHWPOSNEG/input-text2.txt', 'r', encoding='ISO-8859-1') as f:\n",
        "    input_text = f.read()\n",
        "\n",
        "def gen_senti(input_text, path=\"/content/drive/My Drive/NLPHWPOSNEG/\"):\n",
        "    # Load positive words\n",
        "    with open(path + 'positive-words.txt', 'r') as f:\n",
        "        positive_words = set(f.read().splitlines())\n",
        "\n",
        "    # Load negative words\n",
        "    with open(path + 'negative-words.txt', 'r') as f:\n",
        "        negative_words = set(f.read().splitlines())\n",
        "\n",
        "    tokens = word_tokenize(input_text.lower())\n",
        "\n",
        "    # counts and lists to store matched words\n",
        "    pc = 0  # Positive count\n",
        "    nc = 0  # Negative count\n",
        "    matched_positive_words = []  # To store matched positive words\n",
        "    matched_negative_words = []  # To store matched negative words\n",
        "\n",
        "    # positive and negative tokens\n",
        "    for token in tokens:\n",
        "        if token in positive_words:\n",
        "            pc += 1\n",
        "            matched_positive_words.append(token)  # Add to matched positive words\n",
        "        elif token in negative_words:\n",
        "            nc += 1\n",
        "            matched_negative_words.append(token)  # Add to matched negative words\n",
        "\n",
        "    # sentiment score\n",
        "    total_count = pc + nc\n",
        "    if total_count > 0:\n",
        "        S = (pc - nc) / total_count  # sentiment score between -1 and 1\n",
        "    else:\n",
        "        S = 0  # Neutral sentiment if no matched tokens\n",
        "\n",
        "    return S, pc, nc, matched_positive_words, matched_negative_words\n",
        "\n",
        "# function with the loaded input text\n",
        "score, positive_count, negative_count, matched_pos, matched_neg = gen_senti(input_text)\n",
        "print(f\"Sentiment Score: {score}, Positive Count: {positive_count}, Negative Count: {negative_count}\")\n",
        "print(f\"Matched Positive Words: {matched_pos}\")\n",
        "print(f\"Matched Negative Words: {matched_neg}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQq86I0hIFs9",
        "outputId": "e9cd85c4-d866-45e8-89a4-4a04d48b9e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Score: -0.5, Positive Count: 1, Negative Count: 3\n",
            "Matched Positive Words: ['boom']\n",
            "Matched Negative Words: ['cruel', 'desert', 'struggled']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Using the dataframe from the lecture, the_data, column body, apply this function to each corpus and add a column called simple_senti."
      ],
      "metadata": {
        "id": "4crIHnffLBNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "import pandas as pd\n",
        "\n",
        "the_path = \"/content/drive/My Drive/NLP Text Data/\"\n",
        "# Some preliminary cleaning functions\n",
        "def clean_txt(var_in):\n",
        "    import re\n",
        "    # Modify the regex to preserve punctuation and capitalization\n",
        "    tmp_t = re.sub(\"[^A-Za-z0-9!@#\\$%\\^&\\*\\(\\)\\-_\\+=\\.,;:!?'\\\"]+\", \" \", var_in).strip()\n",
        "\n",
        "    return tmp_t\n",
        "\n",
        "def read_file(full_path_in):\n",
        "    with open(full_path_in, \"r\", encoding=\"UTF-8\") as f_t:\n",
        "        text_t = f_t.read() # Reads the entire file\n",
        "        text_t = clean_txt(text_t)\n",
        "    return text_t\n",
        "\n",
        "def file_crawler(path_in):\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    my_pd_t = pd.DataFrame()\n",
        "    for root, dirs, files in os.walk(path_in, topdown=False):\n",
        "        for name in files:\n",
        "            try:\n",
        "                txt_t = read_file(root + \"/\" + name)\n",
        "                if len(txt_t) > 0:\n",
        "                    the_lab = root.split(\"/\")[-1]\n",
        "                    tmp_pd = pd.DataFrame({\"body\": txt_t, \"label\": the_lab}, index=[0])\n",
        "                    my_pd_t = pd.concat([my_pd_t, tmp_pd], ignore_index=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error with file {root}/{name}: {e}\")\n",
        "                pass\n",
        "    return my_pd_t\n",
        "\n",
        "df = file_crawler(the_path)\n",
        "print(\"Sample of the DataFrame:\")\n",
        "print(df.head())  # Print the first few rows of the DataFrame\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu72LuYGRno7",
        "outputId": "68880d71-e157-4850-d65e-e6899ec14ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error with file /content/drive/My Drive/NLP Text Data/fishing/UK MongoBD report Nov122018.xls: 'utf-8' codec can't decode byte 0xd0 in position 0: invalid continuation byte\n",
            "Error with file /content/drive/My Drive/NLP Text Data/fishing/UK segment count Nov122018.xlsx: 'utf-8' codec can't decode bytes in position 15-16: invalid continuation byte\n",
            "Error with file /content/drive/My Drive/NLP Text Data/fishing/UK vendor count Nov122108 .xlsx: 'utf-8' codec can't decode bytes in position 15-16: invalid continuation byte\n",
            "Sample of the DataFrame:\n",
            "                                                body            label\n",
            "0  Machine Learning Total 239.99 Computer Science...  machinelearning\n",
            "1  Rendezvous Server to the Rescue: Dealing with ...  machinelearning\n",
            "2  The 10 Algorithms Machine Learning Engineers N...  machinelearning\n",
            "3  Find a Job in Artificial Intelligence or Machi...  machinelearning\n",
            "4  xkcd: Machine Learning Archive What If? Blag S...  machinelearning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load positive and negative words\n",
        "def load_word_lists(path):\n",
        "    with open(path + 'positive-words.txt', 'r') as f:\n",
        "        positive_words = set(f.read().splitlines())\n",
        "    with open(path + 'negative-words.txt', 'r') as f:\n",
        "        negative_words = set(f.read().splitlines())\n",
        "    return positive_words, negative_words\n",
        "\n",
        "# sentiment analysis function\n",
        "def gen_senti(input_text, positive_words, negative_words):\n",
        "    # Tokenize the input text\n",
        "    tokens = word_tokenize(input_text.lower())\n",
        "\n",
        "    # Initialize counts\n",
        "    pc = 0  # Positive count\n",
        "    nc = 0  # Negative count\n",
        "\n",
        "    # Count positive and negative tokens\n",
        "    for token in tokens:\n",
        "        if token in positive_words:\n",
        "            pc += 1\n",
        "        elif token in negative_words:\n",
        "            nc += 1\n",
        "\n",
        "    # Calculate sentiment score\n",
        "    total_count = pc + nc\n",
        "    if total_count > 0:\n",
        "        S = (pc - nc) / total_count  # Normalized sentiment score between -1 and 1\n",
        "    else:\n",
        "        S = 0  # Neutral sentiment if no matched tokens\n",
        "\n",
        "    return S\n",
        "\n",
        "\n",
        "path = \"/content/drive/My Drive/NLPHWPOSNEG/\"\n",
        "positive_words, negative_words = load_word_lists(path)\n",
        "\n",
        "the_path = \"/content/drive/My Drive/NLP Text Data/\"\n",
        "df = file_crawler(the_path)\n",
        "\n",
        "# Apply sentiment analysis to the 'body' column\n",
        "df['simple_senti'] = df['body'].apply(lambda x: gen_senti(x, positive_words, negative_words))\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(\"Updated DataFrame with Sentiment Scores:\")\n",
        "print(df[['body', 'simple_senti']].head())  # Display the 'body' and 'simple_senti' columns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZh9AjZNRMy6",
        "outputId": "dbfaa2f5-9edd-4a79-c8a1-11d85ecb8bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error with file /content/drive/My Drive/NLP Text Data/fishing/UK MongoBD report Nov122018.xls: 'utf-8' codec can't decode byte 0xd0 in position 0: invalid continuation byte\n",
            "Error with file /content/drive/My Drive/NLP Text Data/fishing/UK segment count Nov122018.xlsx: 'utf-8' codec can't decode bytes in position 15-16: invalid continuation byte\n",
            "Error with file /content/drive/My Drive/NLP Text Data/fishing/UK vendor count Nov122108 .xlsx: 'utf-8' codec can't decode bytes in position 15-16: invalid continuation byte\n",
            "Updated DataFrame with Sentiment Scores:\n",
            "                                                body  simple_senti\n",
            "0  Machine Learning Total 239.99 Computer Science...      0.170732\n",
            "1  Rendezvous Server to the Rescue: Dealing with ...      0.629630\n",
            "2  The 10 Algorithms Machine Learning Engineers N...      0.411765\n",
            "3  Find a Job in Artificial Intelligence or Machi...      0.730769\n",
            "4  xkcd: Machine Learning Archive What If? Blag S...      0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Using vaderSentiment, apply the compound value of sentiment for each corpus in column body on a new column of the_data called vader."
      ],
      "metadata": {
        "id": "MLVewVwjM7Pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and import necessary libraries\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "\n",
        "# VADER lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "#VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "\n",
        "# Function to compute the compound score\n",
        "def compute_vader_sentiment(text):\n",
        "    score = sia.polarity_scores(text)\n",
        "    return score['compound']  # Return the compound score\n",
        "\n",
        "# Apply the function to the 'body' column and create a new 'vader' column\n",
        "df['vader'] = df['body'].apply(compute_vader_sentiment)\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(\"Updated DataFrame with VADER Sentiment Scores:\")\n",
        "print(df[['body', 'vader']].head())  # Display the 'body' and 'vader' columns\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xT5UXNS3V5OK",
        "outputId": "3d288910-8491-42e2-fe8c-276951227025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated DataFrame with VADER Sentiment Scores:\n",
            "                                                body   vader\n",
            "0  Machine Learning Total 239.99 Computer Science...  0.9941\n",
            "1  Rendezvous Server to the Rescue: Dealing with ...  0.9968\n",
            "2  The 10 Algorithms Machine Learning Engineers N...  0.9473\n",
            "3  Find a Job in Artificial Intelligence or Machi...  0.9992\n",
            "4  xkcd: Machine Learning Archive What If? Blag S...  0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Compute the mean, median, and standard deviations of both sentiment measures, simple_senti and vader"
      ],
      "metadata": {
        "id": "NFHCDzRJNPPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# mean, median, and standard deviation for both sentiment measures\n",
        "statistics = {\n",
        "    'simple_senti': {\n",
        "        'mean': df['simple_senti'].mean(),\n",
        "        'median': df['simple_senti'].median(),\n",
        "        'std_dev': df['simple_senti'].std()\n",
        "    },\n",
        "    'vader': {\n",
        "        'mean': df['vader'].mean(),\n",
        "        'median': df['vader'].median(),\n",
        "        'std_dev': df['vader'].std()\n",
        "    }\n",
        "}\n",
        "\n",
        "# results\n",
        "print(\"Sentiment Statistics:\")\n",
        "for measure, stats in statistics.items():\n",
        "    print(f\"{measure} - Mean: {stats['mean']:.4f}, Median: {stats['median']:.4f}, Standard Deviation: {stats['std_dev']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjxI7POXWt2a",
        "outputId": "80fa19cb-f386-496e-ab1e-5769f7040fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Statistics:\n",
            "simple_senti - Mean: 0.3784, Median: 0.4286, Standard Deviation: 0.4338\n",
            "vader - Mean: 0.8758, Median: 0.9851, Standard Deviation: 0.2641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we see that simple_senti and vader differ significantly across all measurements.This is mostly because vader is a lot more flexible and smarter than a simple matching function like simple_senti.\n",
        "\n",
        "Homework 2 concluded\n",
        "####################\n",
        "####################"
      ],
      "metadata": {
        "id": "jSyyf1JlNVnF"
      }
    }
  ]
}