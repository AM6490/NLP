{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#mounting google drive files\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "drive.mount('/content/drive')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5Mkenulyc5k",
        "outputId": "9c82b96c-072f-4a3a-c18d-cfe372f4a400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NLP Text Data folder contains all the folders for the corpuses. The output shows this\n",
        "import os\n",
        "\n",
        "# Path to the directory\n",
        "path = '/content/drive/My Drive/NLP Text Data/'\n",
        "\n",
        "# List files\n",
        "files = os.listdir(path)\n",
        "print(files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9OQVJUuywwA",
        "outputId": "73aa328d-2329-4824-fe6f-356876095f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['machinelearning', 'hiking', 'fishing', 'mathematics']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning and prep along with creating the necessary columns\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "the_path = \"/content/drive/My Drive/NLP Text Data/\"\n",
        "\n",
        "# Clean the original text\n",
        "def clean_txt(var_in):\n",
        "    tmp_t = re.sub(\"[^A-Za-z0-9!@#\\$%\\^&\\*\\(\\)\\-_\\+=\\.,;:!?'\\\"]+\", \" \", var_in).strip()\n",
        "    return tmp_t\n",
        "\n",
        "# Remove stopwords from the text\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    return ' '.join([word for word in words if word.lower() not in stop_words])\n",
        "\n",
        "# Apply stemming to the text\n",
        "def apply_stemming(text):\n",
        "    words = text.split()\n",
        "    return ' '.join([stemmer.stem(word) for word in words])\n",
        "\n",
        "# Read and process each file\n",
        "def read_file(full_path_in):\n",
        "    with open(full_path_in, \"r\", encoding=\"UTF-8\") as f_t:\n",
        "        text_t = f_t.read()  # Reads the entire file\n",
        "        text_t = clean_txt(text_t)  # Basic cleaning\n",
        "    return text_t\n",
        "\n",
        "# Main function to create DataFrame with all columns\n",
        "def file_crawler(path_in):\n",
        "    my_pd_t = pd.DataFrame()\n",
        "\n",
        "    for root, dirs, files in os.walk(path_in, topdown=False):\n",
        "        for name in files:\n",
        "            try:\n",
        "                # Read and clean text\n",
        "                txt_t = read_file(root + \"/\" + name)\n",
        "                if len(txt_t) > 0:\n",
        "                    the_lab = root.split(\"/\")[-1]\n",
        "\n",
        "                    # Create different versions of the text\n",
        "                    body = txt_t\n",
        "                    body_sw = remove_stopwords(txt_t)\n",
        "                    body_sw_stem = apply_stemming(body_sw)\n",
        "\n",
        "                    # Add to DataFrame\n",
        "                    tmp_pd = pd.DataFrame({\n",
        "                        \"body\": body,\n",
        "                        \"body_sw\": body_sw,\n",
        "                        \"body_sw_stem\": body_sw_stem,\n",
        "                        \"topic\": the_lab\n",
        "                    }, index=[0])\n",
        "\n",
        "                    my_pd_t = pd.concat([my_pd_t, tmp_pd], ignore_index=True)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error with file {root}/{name}: {e}\")\n",
        "                pass\n",
        "\n",
        "    return my_pd_t\n",
        "\n",
        "# Create the DataFrame\n",
        "the_data = file_crawler(the_path)\n",
        "print(\"Sample of the DataFrame:\")\n",
        "print(the_data.head())  # Print the first few rows of the DataFrame\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjfNIM5-1Rra",
        "outputId": "708674f9-e82d-4991-933e-5bdd1c343379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error with file /content/drive/My Drive/NLP Text Data/fishing/UK MongoBD report Nov122018.xls: 'utf-8' codec can't decode byte 0xd0 in position 0: invalid continuation byte\n",
            "Error with file /content/drive/My Drive/NLP Text Data/fishing/UK segment count Nov122018.xlsx: 'utf-8' codec can't decode bytes in position 15-16: invalid continuation byte\n",
            "Error with file /content/drive/My Drive/NLP Text Data/fishing/UK vendor count Nov122108 .xlsx: 'utf-8' codec can't decode bytes in position 15-16: invalid continuation byte\n",
            "Sample of the DataFrame:\n",
            "                                                body  \\\n",
            "0  Machine Learning Total 239.99 Computer Science...   \n",
            "1  Rendezvous Server to the Rescue: Dealing with ...   \n",
            "2  The 10 Algorithms Machine Learning Engineers N...   \n",
            "3  Find a Job in Artificial Intelligence or Machi...   \n",
            "4  xkcd: Machine Learning Archive What If? Blag S...   \n",
            "\n",
            "                                             body_sw  \\\n",
            "0  Machine Learning Total 239.99 Computer Science...   \n",
            "1  Rendezvous Server Rescue: Dealing Machine Lear...   \n",
            "2  10 Algorithms Machine Learning Engineers Need ...   \n",
            "3  Find Job Artificial Intelligence Machine Learn...   \n",
            "4  xkcd: Machine Learning Archive If? Blag Store ...   \n",
            "\n",
            "                                        body_sw_stem            topic  \n",
            "0  machin learn total 239.99 comput scienc artifi...  machinelearning  \n",
            "1  rendezv server rescue: deal machin learn logis...  machinelearning  \n",
            "2  10 algorithm machin learn engin need know kdnu...  machinelearning  \n",
            "3  find job artifici intellig machin learn busi i...  machinelearning  \n",
            "4  xkcd: machin learn archiv if? blag store prev ...  machinelearning  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing for the token fishing\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def word_prob(column_name, the_data, token=\"fishing\", decimals=4):\n",
        "    # Initialize dictionary to store probabilities for the \"fishing\" token\n",
        "    probabilities = {\"all\": None, \"fishing\": None, \"hiking\": None, \"machinelearning\": None, \"mathematics\": None}\n",
        "\n",
        "    # Prepare token pattern for matching\n",
        "    token_pattern = re.escape(token) if \" \" in token else r'\\b' + re.escape(token) + r'\\b'\n",
        "\n",
        "    # Calculate probability for \"all\" (entire dataset)\n",
        "    all_tokens = the_data[column_name].str.findall(r'\\b\\w+\\b').apply(len).sum()\n",
        "    count_token_all = the_data[column_name].str.count(token_pattern).sum()\n",
        "    probabilities[\"all\"] = round(count_token_all / all_tokens, decimals) if all_tokens > 0 else None\n",
        "\n",
        "    # Calculate probability for each topic\n",
        "    topics = [\"fishing\", \"hiking\", \"machinelearning\", \"mathematics\"]\n",
        "    for topic in topics:\n",
        "        topic_df = the_data[the_data[\"topic\"] == topic]\n",
        "        total_tokens_topic = topic_df[column_name].str.findall(r'\\b\\w+\\b').apply(len).sum()\n",
        "        count_token_topic = topic_df[column_name].str.count(r'\\b' + re.escape(token) + r'\\b').sum()\n",
        "        probabilities[topic] = round(count_token_topic / total_tokens_topic, decimals) if total_tokens_topic > 0 else None\n",
        "\n",
        "    # Print and return the probabilities dictionary\n",
        "    print(probabilities)\n",
        "    return probabilities\n",
        "\n",
        "result = word_prob(\"body\", the_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwBi9qh_-L7q",
        "outputId": "cadfbce5-b3a2-442d-e7d1-4960665b7f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'all': 0.0012, 'fishing': 0.0057, 'hiking': 0.0001, 'machinelearning': 0.0, 'mathematics': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing for the token machine learning\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def word_prob(column_name, the_data, token=\"machine learning\", decimals=4):\n",
        "    # Initialize dictionary to store probabilities for the \"fishing\" token\n",
        "    probabilities = {\"all\": None, \"fishing\": None, \"hiking\": None, \"machinelearning\": None, \"mathematics\": None}\n",
        "\n",
        "    # Prepare token pattern for matching\n",
        "    token_pattern = re.escape(token) if \" \" in token else r'\\b' + re.escape(token) + r'\\b'\n",
        "\n",
        "    # Calculate probability for \"all\" (entire dataset)\n",
        "    all_tokens = the_data[column_name].str.findall(r'\\b\\w+\\b').apply(len).sum()\n",
        "    count_token_all = the_data[column_name].str.count(token_pattern).sum()\n",
        "    probabilities[\"all\"] = round(count_token_all / all_tokens, decimals) if all_tokens > 0 else None\n",
        "\n",
        "    # Calculate probability for each topic\n",
        "    topics = [\"fishing\", \"hiking\", \"machinelearning\", \"mathematics\"]\n",
        "    for topic in topics:\n",
        "        topic_df = the_data[the_data[\"topic\"] == topic]\n",
        "        total_tokens_topic = topic_df[column_name].str.findall(r'\\b\\w+\\b').apply(len).sum()\n",
        "        count_token_topic = topic_df[column_name].str.count(r'\\b' + re.escape(token) + r'\\b').sum()\n",
        "        probabilities[topic] = round(count_token_topic / total_tokens_topic, decimals) if total_tokens_topic > 0 else None\n",
        "\n",
        "    # Print and return the probabilities dictionary\n",
        "    print(probabilities)\n",
        "    return probabilities\n",
        "\n",
        "result = word_prob(\"body\", the_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyW0n4bf-cqF",
        "outputId": "2200a33f-37ff-4ad1-977b-23a8ad329e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'all': 0.0017, 'fishing': 0.0, 'hiking': 0.0, 'machinelearning': 0.006, 'mathematics': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing for the token mathematics\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def word_prob(column_name, the_data, token=\"mathematics\", decimals=4):\n",
        "    # Initialize dictionary to store probabilities for the \"fishing\" token\n",
        "    probabilities = {\"all\": None, \"fishing\": None, \"hiking\": None, \"machinelearning\": None, \"mathematics\": None}\n",
        "\n",
        "    # Prepare token pattern for matching\n",
        "    token_pattern = re.escape(token) if \" \" in token else r'\\b' + re.escape(token) + r'\\b'\n",
        "\n",
        "    # Calculate probability for \"all\" (entire dataset)\n",
        "    all_tokens = the_data[column_name].str.findall(r'\\b\\w+\\b').apply(len).sum()\n",
        "    count_token_all = the_data[column_name].str.count(token_pattern).sum()\n",
        "    probabilities[\"all\"] = round(count_token_all / all_tokens, decimals) if all_tokens > 0 else None\n",
        "\n",
        "    # Calculate probability for each topic\n",
        "    topics = [\"fishing\", \"hiking\", \"machinelearning\", \"mathematics\"]\n",
        "    for topic in topics:\n",
        "        topic_df = the_data[the_data[\"topic\"] == topic]\n",
        "        total_tokens_topic = topic_df[column_name].str.findall(r'\\b\\w+\\b').apply(len).sum()\n",
        "        count_token_topic = topic_df[column_name].str.count(r'\\b' + re.escape(token) + r'\\b').sum()\n",
        "        probabilities[topic] = round(count_token_topic / total_tokens_topic, decimals) if total_tokens_topic > 0 else None\n",
        "\n",
        "    # Print and return the probabilities dictionary\n",
        "    print(probabilities)\n",
        "    return probabilities\n",
        "\n",
        "result = word_prob(\"body\", the_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P52oVr7e-hrI",
        "outputId": "3916c0b4-b69b-453a-ccbc-c2d166682ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'all': 0.0008, 'fishing': 0.0, 'hiking': 0.0, 'machinelearning': 0.0001, 'mathematics': 0.0036}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing for the token hiking\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def word_prob(column_name, the_data, token=\"hiking\", decimals=4):\n",
        "    # Initialize dictionary to store probabilities for the \"fishing\" token\n",
        "    probabilities = {\"all\": None, \"fishing\": None, \"hiking\": None, \"machinelearning\": None, \"mathematics\": None}\n",
        "\n",
        "    # Prepare token pattern for matching\n",
        "    token_pattern = re.escape(token) if \" \" in token else r'\\b' + re.escape(token) + r'\\b'\n",
        "\n",
        "    # Calculate probability for \"all\" (entire dataset)\n",
        "    all_tokens = the_data[column_name].str.findall(r'\\b\\w+\\b').apply(len).sum()\n",
        "    count_token_all = the_data[column_name].str.count(token_pattern).sum()\n",
        "    probabilities[\"all\"] = round(count_token_all / all_tokens, decimals) if all_tokens > 0 else None\n",
        "\n",
        "    # Calculate probability for each topic\n",
        "    topics = [\"fishing\", \"hiking\", \"machinelearning\", \"mathematics\"]\n",
        "    for topic in topics:\n",
        "        topic_df = the_data[the_data[\"topic\"] == topic]\n",
        "        total_tokens_topic = topic_df[column_name].str.findall(r'\\b\\w+\\b').apply(len).sum()\n",
        "        count_token_topic = topic_df[column_name].str.count(r'\\b' + re.escape(token) + r'\\b').sum()\n",
        "        probabilities[topic] = round(count_token_topic / total_tokens_topic, decimals) if total_tokens_topic > 0 else None\n",
        "\n",
        "    # Print and return the probabilities dictionary\n",
        "    print(probabilities)\n",
        "    return probabilities\n",
        "\n",
        "result = word_prob(\"body\", the_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAipcX8j-z3F",
        "outputId": "6c33e82c-50c2-469a-defa-ea5f3c33d93d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'all': 0.0007, 'fishing': 0.0, 'hiking': 0.0025, 'machinelearning': 0.0, 'mathematics': 0.0}\n"
          ]
        }
      ]
    }
  ]
}